# -*- coding: utf-8 -*-
"""Final Project Big Data - Scraping Youtube.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1giLgPASGeWVReqPFs4pykkO8pykx2Ouy

# **Download Requirement Library**
"""

# Install atau upgrade paket-paket yang diperlukan menggunakan pip
!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib
!pip install googletrans

# Menggunakan perintah dari IPython untuk membersihkan output terminal agar tampilan lebih bersih
from IPython.display import clear_output
clear_output()

"""# **Import Requirement Library**"""

import matplotlib.pyplot as plt  # Import modul untuk membuat visualisasi menggunakan Matplotlib
import re  # Import modul untuk operasi regular expression
import nltk  # Import Natural Language Toolkit (NLTK) untuk pemrosesan bahasa alami
import os  # Import modul os untuk interaksi dengan sistem operasi
import pandas as pd  # Import modul pandas untuk manipulasi data dalam bentuk DataFrame
import matplotlib.pyplot as plt  # Import kembali modul Matplotlib (ganda)

from time import sleep  # Import fungsi sleep dari modul time untuk mengatur waktu tunggu
from textblob import TextBlob  # Import kelas TextBlob dari modul textblob untuk analisis sentimen
from google.colab import drive  # Import fungsi drive dari modul google.colab untuk interaksi dengan Google Drive
from wordcloud import WordCloud  # Import kelas WordCloud dari modul wordcloud untuk membuat word cloud
from nltk.corpus import stopwords  # Import corpus stop words dari NLTK
from transformers import pipeline  # Import pipeline dari modul transformers untuk analisis sentimen menggunakan BERT
from googletrans import Translator  # Import kelas Translator dari modul googletrans untuk terjemahan teks
from nltk.tokenize import word_tokenize  # Import fungsi word_tokenize dari NLTK untuk tokenisasi kata
from urllib.parse import urlparse, parse_qs  # Import fungsi-fungsi untuk memecah URL
from googleapiclient.discovery import build  # Import fungsi build dari modul googleapiclient untuk membangun objek API
from googleapiclient.errors import HttpError  # Import kelas HttpError dari modul googleapiclient.errors untuk penanganan kesalahan HTTP
from os import listdir, makedirs, remove, path  # Import beberapa fungsi dari modul os untuk manipulasi file dan direktori
from pandas import DataFrame, to_datetime, read_excel  # Import beberapa fungsi dan kelas dari modul pandas

clear_output()

"""## **Import Requirement Library for NLTK**"""

nltk.download('punkt')  # Mengunduh data Punkt untuk tokenisasi kata dari NLTK
nltk.download('stopwords')  # Mengunduh data stop words dari NLTK

clear_output()

"""# **Configuration YouTube API Client**"""

# API untuk Scrape Youtube yang didapatkan dari Google Cloud Console YouTube Data API v3
API_KEY = "AIzaSyDELjLXMI1qfxhwWLxs4OGRZ_uvvrGL8y8"
youtube = build('youtube', 'v3', developerKey=API_KEY)

"""# **Scrapping Channel**

Berisi logika untuk proses *scraping* channel youtube

### **Set Display Banner**
"""

def banner(options):
  if options == 1:
    print('''
 _  _ _____ __  __ ____ __  __ ____ ____    ___  ___ ____   __   ____ ____ ____
( \/ (  _  (  )(  (_  _(  )(  (  _ ( ___)  / __)/ __(  _ \ /__\ (  _ ( ___(  _ \\
 \  / )(_)( )(__)(  )(  )(__)( ) _ <)__)   \__ ( (__ )   //(__)\ )___/)__) )   /
 (__)(_____(______)(__)(______(____(____)  (___/\___(_)\_(__)(__(__) (____(_)\_)
================================================================================''')
  elif options == 2:
    print(''' ___ __  __ ___ ___ ____ ___
/ __(  )(  / __/ __( ___/ __)
\__ \)(__)( (_( (__ )__)\__ \\
(___(______\___\___(____(___/
''')
  else:
    print("")

"""## **Proccess Scrapping Channel**"""

banner(1)
channel_name = input("[#] Masukkan nama channel YouTube: ")

# Mendapatkan ID channel
response = youtube.search().list(q=channel_name, type='channel', part='id', maxResults=1).execute()

if 'items' in response and response['items']:
    channel_id = response['items'][0]['id']['channelId']

    # Menggunakan channel ID untuk mendapatkan statistik channel
    response = youtube.channels().list(part='snippet, statistics', id=channel_id).execute()

    if 'items' in response and response['items']:
        channel_title = response['items'][0]['snippet']['title']
        channel_link = f"https://www.youtube.com/channel/{channel_id}"
        channel_data = response['items'][0]['statistics']

        # Menampilkan statistik
        print(f"    > Nama Channel     : {channel_title}")
        print(f"    > ID Channel       : {channel_id}")
        print(f"    > Tautan Channel   : {channel_link}")
        print(f"    > Jumlah Tayangan  : {format(int(channel_data['viewCount']), ',').replace(',', '.')}")
        print(f"    > Jumlah Pelanggan : {format(int(channel_data['subscriberCount']), ',').replace(',', '.')}")
        print(f"    > Jumlah Video     : {format(int(channel_data['videoCount']), ',').replace(',', '.')}")

        # Membuat DataFrame dari informasi yang terkumpul
        df = DataFrame({
            'Channel Name': [channel_title],
            'Channel ID': [channel_id],
            'Channel Link': [channel_link],
            'View Count': [channel_data['viewCount']],
            'Subscriber Count': [channel_data['subscriberCount']],
            'Video Count': [channel_data['videoCount']]
        })

        # Membuat direktori untuk channel
        channel_directory = f"Hasil_Scraping/{channel_title.replace(' ', '_')}"
        makedirs(channel_directory, exist_ok=True)

        # Menyimpan DataFrame ke file Excel dalam direktori channel
        excel_file_path = path.join(channel_directory, f"DataChannel_{channel_title.replace(' ', '_')}.xlsx")
    else:
        print(f"Channel '{channel_name}' tidak ditemukan.")
else:
    print(f"Channel '{channel_name}' tidak ditemukan.")

"""## **Proccess Save Result File into Google Colab Directory**"""

try:
    # Menyimpan DataFrame ke file Excel tanpa menyertakan indeks
    df.to_excel(excel_file_path, index=False)

    # Menampilkan pesan sukses dengan menggunakan banner dan memberikan tautan yang diformat secara khusus
    banner(2)
    print(f'Data berhasil disimpan ke \'\x1B[3m{excel_file_path}\033[0m\'')
except Exception as e:
    # Menampilkan pesan error jika terjadi kesalahan saat menyimpan
    print(f'Error saving the file: {e}')

"""# **Scrapping Video from Channel**

Berisi logika untuk proses *scrapping* video berdasarkan channel yang telah di*scrapping*

## **Initialize Function**

*   parse_duration()
"""

def parse_duration(duration):
    # Ekstrak jam, menit, dan detik dari format durasi ISO 8601
    match = re.match(r'PT(\d+H)?(\d+M)?(\d+S)?', duration)
    if match:
        # Ambil nilai jam, jika tidak ada, set menjadi 0
        hours = int(match.group(1)[:-1]) if match.group(1) else 0
        # Ambil nilai menit, jika tidak ada, set menjadi 0
        minutes = int(match.group(2)[:-1]) if match.group(2) else 0
        # Ambil nilai detik, jika tidak ada, set menjadi 0
        seconds = int(match.group(3)[:-1]) if match.group(3) else 0
        # Konversi durasi ke dalam detik
        total_seconds = hours * 3600 + minutes * 60 + seconds
        return total_seconds
    else:
        return 0

"""## **Initialize Variable**

Variabel-variabel ini digunakan untuk menyimpan informasi tentang video YouTube yang akan diekstrak, seperti judul, deskripsi, tanggal publikasi, jumlah tampilan, jumlah suka, jumlah komentar, tautan video, tautan thumbnail, dan durasi video.
"""

next_page_token = None
max_results = 100
total_results = 0

video_titles = []
video_descriptions = []
video_publish_dates = []
video_publish_times = []
video_view_counts = []
video_like_counts = []
video_comment_counts = []
video_links = []
video_thumbnail_links = []
video_durations = []

"""## **Proccess Scrapping Video from Channel**

Dibawah ini merupakan kode untuk mengambil informasi video dari suatu saluran YouTube, termasuk judul, deskripsi, tanggal unggah, jumlah tayang, jumlah suka, jumlah komentar, durasi, tautan video, dan tautan thumbnail. Data tersebut kemudian disimpan dalam DataFrame dan disimpan ke file Excel.
"""

# Ambil video dari saluran
while True:
    request = youtube.search().list(part='snippet', channelId=channel_id, maxResults=min(50, max_results - total_results), pageToken=next_page_token)
    response = request.execute()

    if 'items' in response:
        for item in response['items']:
            if 'id' in item and 'videoId' in item['id']:
                video_id = item['id']['videoId']
                video_title = item['snippet']['title']
                channel_name = item['snippet']['channelTitle']
                cleaned_title = video_title.replace(channel_name, '').strip()
                if cleaned_title:
                    video_titles.append(cleaned_title)

                    # Ambil detail video termasuk durasi
                    video_response = youtube.videos().list(part='snippet,statistics,contentDetails', id=video_id).execute()
                    video_snippet = video_response['items'][0]['snippet']
                    video_statistics = video_response['items'][0]['statistics']
                    video_content_details = video_response['items'][0]['contentDetails']

                    video_descriptions.append(video_snippet.get('description', 'None'))
                    video_publish_date = to_datetime(video_snippet['publishedAt'])  # Konversi ke datetime
                    video_view_counts.append(video_statistics.get('viewCount', 0))
                    video_like_counts.append(video_statistics.get('likeCount', 0))
                    video_comment_counts.append(video_statistics.get('commentCount', 0))  # Ambil jumlah komentar
                    video_links.append(f'https://www.youtube.com/watch?v={video_id}')
                    video_thumbnail_links.append(video_snippet['thumbnails']['default']['url'])

                    # Dapatkan durasi video
                    video_duration = video_content_details['duration']
                    video_duration_seconds = parse_duration(video_duration)
                    video_durations.append(video_duration_seconds)

                    # Tambahkan tanggal dan waktu unggah
                    video_publish_dates.append(video_publish_date.strftime('%Y-%m-%d'))
                    video_publish_times.append(video_publish_date.strftime('%H:%M:%S'))

        total_results += len(response['items'])
        next_page_token = response.get('nextPageToken')

        if not next_page_token or total_results >= max_results:
            break

# Buat DataFrame dari informasi yang dikumpulkan
df = DataFrame({
    'Judul': video_titles,
    'Deskripsi': video_descriptions,
    'Tanggal Upload': video_publish_dates,
    'Jam Upload': video_publish_times,
    'Jumlah Tayang': video_view_counts,
    'Jumlah Like': video_like_counts,
    'Jumlah Komentar': video_comment_counts,
    'Durasi': video_durations,
    'Link Video': video_links,
    'Link Thumbnail': video_thumbnail_links
})

# Buat direktori untuk saluran
channel_directory = f"Hasil_Scraping/{channel_name.replace(' ', '_')}"
makedirs(channel_directory, exist_ok=True)

# Simpan DataFrame ke file Excel dalam direktori saluran
excel_file_path = path.join(channel_directory, f"DataVideo_{channel_name.replace(' ', '_')}.xlsx")

"""## **Proccess Save Result File into Google Colab Directory**"""

try:
    # Simpan DataFrame ke file Excel
    df.to_excel(excel_file_path, index=False)
    banner(2)
    print(f'Data Video dari channel \'\033[1m{channel_name}\033[0m berhasil disimpan ke \x1B[3m{excel_file_path}\033[0m\'')
except Exception as e:
    # Tampilkan pesan kesalahan jika terjadi
    print(f'Error saving the file: {e}')

"""# **Proccess Scrapping Comment from Selected Video**

Berisi logika untuk proses scrapping komentar berdasarkan video yang telah discrapping dari suatu channel

## **Initialize Function**

*   scrape_comments_from_video(link, max_comments=500)
Fungsi ini mengambil komentar dari suatu video YouTube berdasarkan tautan video yang diberikan. Parameter link adalah tautan video, dan max_comments (opsional) adalah batas maksimum komentar yang akan diambil.

*   scrape_comments_from_top_videos(channel_id, top_n):
Fungsi ini mengambil informasi tentang N video teratas dari suatu saluran YouTube berdasarkan ID saluran. Parameter channel_id adalah ID saluran YouTube, dan top_n adalah jumlah video teratas yang akan diambil.

*   display_videos_and_get_input(current_index, items_per_page):
Fungsi ini menampilkan N video teratas dari channel YouTube dan meminta input pengguna untuk memilih video. Fungsi ini menggunakan fungsi scrape_comments_from_top_videos untuk mendapatkan informasi video teratas dan kemudian menampilkan video sesuai dengan indeks halaman dan jumlah video per halaman.
"""

# Fungsi untuk mendapatkan komentar dari suatu video
def scrape_comments_from_video(link, max_comments=500):
    video_id = parse_qs(urlparse(link).query).get('v', [None])[0]

    if video_id:
        comments_data = []
        page_token = None

        while len(comments_data) < max_comments:
            # Ambil komentar dengan pagination
            video_request = youtube.commentThreads().list(
                part='snippet,replies',
                videoId=video_id,
                textFormat='plainText',
                maxResults=min(100, max_comments - len(comments_data)),
                pageToken=page_token
            )
            video_response = video_request.execute()

            for item in video_response['items']:
                snippet = item['snippet']['topLevelComment']['snippet']
                profile_link = f"https://www.youtube.com/channel/{snippet['authorChannelId']['value']}"
                username = snippet['authorDisplayName']
                timestamp = to_datetime(snippet['publishedAt'])
                comment_date = timestamp.strftime('%Y-%m-%d')
                comment_time = timestamp.strftime('%H:%M:%S')
                comment_text = snippet['textDisplay']
                like_count = snippet['likeCount']

                # Ambil jumlah balasan
                reply_count = item['snippet']['totalReplyCount']

                # Ekstrak ID komentar
                comment_id = item['id']

                # Additional API request untuk mendapatkan foto profil pengguna
                channel_info = youtube.channels().list(
                    part='snippet',
                    id=snippet['authorChannelId']['value']
                ).execute()

                if 'items' in channel_info and channel_info['items']:
                    profile_photo_url = channel_info['items'][0]['snippet']['thumbnails']['default']['url']
                else:
                    profile_photo_url = 'Not Available'

                comments_data.append({
                    'Comment ID': comment_id,
                    'Profile Link': profile_link,
                    'Profile Photo': profile_photo_url,
                    'Username': username,
                    'Comment Date': comment_date,
                    'Comment Time': comment_time,
                    'Comment Text': comment_text,
                    'Like Count': like_count,
                    'Reply Count': reply_count,  # Termasuk jumlah balasan
                })

            # Periksa untuk pagination
            if 'nextPageToken' in video_response:
                page_token = video_response['nextPageToken']
            else:
                break

        return comments_data[:max_comments]  # Hanya kembalikan hingga max_comments
    else:
        print("ID video tidak ditemukan dalam tautan yang diberikan.")
        return []

# Fungsi untuk mendapatkan komentar dari N video teratas
def scrape_comments_from_top_videos(channel_id, top_n):
    request = youtube.search().list(part='snippet', channelId=channel_id, order='viewCount', type='video', maxResults=top_n)
    response = request.execute()

    videos = {}
    for i, item in enumerate(response['items']):
        video_id = item['id']['videoId']
        video_title = item['snippet']['title']
        videos[i + 1] = {
            'video_id': video_id,
            'video_title': video_title,
        }

    return videos

# Fungsi untuk menampilkan video dan mendapatkan input pengguna
def display_videos_and_get_input(current_index, items_per_page):
    # Hitung rentang item yang akan ditampilkan
    start_index = current_index * items_per_page + 1
    end_index = (current_index + 1) * items_per_page

    top_n = end_index  # Ganti ini sesuai dengan jumlah video teratas yang diinginkan
    top_videos = scrape_comments_from_top_videos(channel_id, top_n)

    # Bersihkan layar
    clear_output(wait=True)

    # Tampilkan video teratas dengan rentang yang benar
    print(f"[>] Video Teratas ({start_index}-{min(end_index, len(top_videos))}):")
    for index, video_info in list(top_videos.items())[start_index - 1:end_index]:
        print(f" |  [{index}] {video_info['video_title']}")

    # Dapatkan input pengguna untuk pemilihan video
    selected_index = input(f"[!] Pilih nomor video yang akan diambil komentarnya (n untuk ke halaman berikutnya, p untuk ke halaman sebelumnya): [{start_index}..{end_index}] ")
    print("===========================================================")

    return selected_index, top_videos

"""## **Proccess Get Comment from Selected Video**"""

# Inisialisasi user_choice untuk selalu memilih opsi pertama
user_choice = 1

# Inisialisasi current_index untuk melacak halaman saat ini
current_index = 0
items_per_page = 10

tmp = 1
temp = 10

while user_choice == 1:
    # Gunakan opsi pertama langsung tanpa meminta input dari pengguna
    print(f"Scrape komentar dengan data dari channel {channel_name}")

    # Dapatkan input pengguna dan tampilkan video
    selected_index, top_videos = display_videos_and_get_input(current_index, items_per_page)

    # Handle input pengguna
    if selected_index == 'n':
        current_index += 1
        tmp += 10
        temp += 10
    elif selected_index == 'p':
        if current_index != 0:
          tmp -= 10
          temp -= 10
        current_index = max(0, current_index - 1)
    else:
        try:
            selected_index = int(selected_index)
            if tmp <= selected_index <= temp:
                # Proses video yang dipilih
                selected_video_info = top_videos.get(selected_index)
                if selected_video_info:
                    print(f"[>] Preview Video yang Dipilih:")
                    print(f"    > Judul: {selected_video_info['video_title']}\n")
                    video_link = f"https://www.youtube.com/watch?v={selected_video_info['video_id']}"
                    comments_data = scrape_comments_from_video(video_link)

                    # Buat DataFrame dari data komentar yang dikumpulkan
                    comments_df = DataFrame(comments_data)

                    # Buat direktori untuk channel
                    channel_directory = f"Hasil_Scraping/{channel_name.replace(' ', '_')}"
                    makedirs(channel_directory, exist_ok=True)

                    # Simpan DataFrame ke file Excel dalam direktori channel
                    excel_file_path = path.join(channel_directory, f"DataComments_{channel_name.replace(' ', '_')}.xlsx")
                    comments_df.to_excel(excel_file_path, index=False)

                    banner(2)
                    print(f"Scraping komentar berhasil disimpan ke '{excel_file_path}'")
                    break  # Keluar dari loop setelah scraping berhasil
                else:
                    print("[!] Nomor video tidak valid.")
                    sleep(3)
            else:
                print(f"    [!] ERROR: Pilihan tidak valid, harap masukkan angka antara {tmp} dan {temp}.")
                sleep(5)
        except ValueError:
            print("    [!] ERROR: Pilihan tidak valid, harap masukkan nomor atau n/p.")
            sleep(3)

"""# **Text Cleaning from Result of Scrapping Comment**

Berisi logika untuk menghapus / menghilangkan beberapa text supaya bersih untuk tujuan analisa nantinya

## **Initialize Function**

*   clean_and_analyze_data(file_path, channel_name)
Fungsi ini membersihkan data komentar YouTube dari file Excel, mengekstrak kolom-kolom penting, membersihkan teks komentar menggunakan fungsi '*clean_comment*', dan menyimpan hasilnya dalam sebuah file Excel baru dengan struktur direktori yang sesuai.

*   clean_comment(comment)
Fungsi ini membersihkan teks komentar dengan menghapus tautan, karakter khusus, dan simbol, melakukan tokenisasi kata, menghapus stop words, menghilangkan karakter berulang, mengubah huruf menjadi huruf kecil, dan menggabungkan kembali kata-kata yang telah dibersihkan.
"""

# Definisi fungsi clean_and_analyze_data
def clean_and_analyze_data(file_path, channel_name):
    # Membaca DataFrame dari file Excel
    df = read_excel(file_path)

    # Lakukan pembersihan data di sini sesuai kebutuhan
    df_cleaned = df[['Comment ID', 'Profile Link', 'Username', 'Comment Date', 'Comment Text']].copy()

    # Membersihkan teks komentar
    df_cleaned['Cleaned Comment'] = df_cleaned['Comment Text'].apply(clean_comment)

    # Buat direktori untuk channel
    channel_directory = f"Hasil_Scraping/{channel_name.replace(' ', '_')}"
    makedirs(channel_directory, exist_ok=True)

    # Simpan DataFrame ke file Excel dalam direktori channel
    excel_file_path = path.join(channel_directory, f"DataCleaning_{channel_name.replace(' ', '_')}.xlsx")
    df_cleaned.to_excel(excel_file_path, index=False)

    print(f"Data yang sudah di cleaning berhasil disimpan ke {excel_file_path}")

# Fungsi untuk membersihkan komentar
def clean_comment(comment):
    if isinstance(comment, str):  # Check if comment is a valid string
        # Hapus tautan (URL)
        comment = re.sub(r'http\S+', '', comment)

        # Hapus karakter khusus dan simbol
        comment = re.sub(r'[^a-zA-Z\s]', '', comment)

        # Tokenisasi kata-kata
        words = word_tokenize(comment)

        # Hapus stop words (kata-kata umum yang tidak memberikan banyak informasi)
        stop_words = set(stopwords.words('english'))
        words = [word for word in words if word.lower() not in stop_words]

        # Hapus karakter berulang
        words = [re.sub(r'(.)\1+', r'\1', word) for word in words]

        # Ubah huruf besar menjadi huruf kecil
        words = [word.lower() for word in words]

        # Gabungkan kata-kata yang sudah dibersihkan kembali menjadi teks
        cleaned_comment = ' '.join(words)

        return cleaned_comment
    else:
        return ''  # Return an empty string if the input is not a valid string

"""## **Proccess Cleaning Text from Comment**"""

# Mendapatkan daftar channel dari direktori Hasil_Scrapping
hasil_scrapping_directory = "Hasil_Scraping"
channel_list = [channel for channel in listdir(hasil_scrapping_directory) if path.isdir(path.join(hasil_scrapping_directory, channel))]

# Validasi agar nomor yang dimasukkan berada dalam rentang yang valid
while True:
    try:
        # Menampilkan daftar channel kepada pengguna
        print("[>] Daftar Channel yang Tersedia: ")
        for idx, channel in enumerate(channel_list, start=1):
          print(f" |  [{idx}] {channel}")
        channel_index = int(input("[!] Masukkan nomor channel yang akan dianalisis: "))
        print("===========================================================")
        if 1 <= channel_index <= len(channel_list):
            break
        else:
            print("[!] ERROR: Mohon masukkan nomor yang tertera diatas!")
            sleep(3)
            clear_output()
    except ValueError:
        print("===========================================================")
        print("[!] ERROR: Mohon masukkan nomor yang valid.")
        sleep(3)
        clear_output()

# Memilih nama channel berdasarkan input pengguna
selected_channel = channel_list[channel_index - 1]
print(f"Anda memilih untuk menganalisis channel: {selected_channel}")

# Bentuk path ke file Excel berdasarkan nama channel yang dipilih
excel_file_path = path.join(hasil_scrapping_directory, selected_channel, f"DataComments_{selected_channel}.xlsx")

# Periksa apakah file Excel tersebut ada
if path.exists(excel_file_path):
    # Lakukan pembersihan dan analisis data
    clean_and_analyze_data(excel_file_path, selected_channel)
else:
    print(f"File Excel tidak ditemukan untuk channel: '{selected_channel}'")

"""# **Sentiment Analysis from Result of Text Cleaning**

Berisi logika untuk melakukan analisa dari hasil text yang sudah dibersihkan

## **Initialize Function**

*   perform_sentiment_analysis()
melakukan analisis sentimen pada teks menggunakan TextBlob, mengembalikan label sentimen ('Positif', 'Negatif', atau 'Netral').

*   add_sentiment_columns()
menambahkan dua kolom ke DataFrame: 'Sentimen' dan 'Sentimen_Encoded'. 'Sentimen' diisi hasil analisis sentimen pada 'Cleaned Comment', sementara 'Sentimen_Encoded' mengkodekan label sentimen.

*   calculate_sentiment_score()
menghitung skor sentimen komentar berdasarkan kamus kata-kata positif dan negatif.

*   translate_text()
menerjemahkan teks ke bahasa target menggunakan Google Translate dan mengembalikan teks terjemahan.

## **Initialize Variable**
"""

# Kamus kata-kata positif dan negatif beserta bobotnya
sentiment_weights = {
    1: ['positive', 'good', 'happy', 'excellent', 'love', 'awesome', 'amazing', 'beautiful', 'wonderful', 'fantastic', 'terrific', 'nice', 'great', 'outstanding', 'respect', 'superb', 'delightful', 'joyful', 'cheerful', 'splendid', 'best', 'top-notch', 'exemplary', 'pleasing', 'grateful', 'glorious', 'uplifting', 'magnificent', 'vibrant', 'lovable', 'exuberant', 'remarkable', 'satisfying', 'admirable', 'stellar', 'heartwarming', 'blissful', 'likely', 'ecstatic', 'jovial', 'divine', 'positive_vibes', 'paradise', 'celebration', 'kudos', 'commendable'],
    -1: ['negative', 'bad', 'sad', 'terrible', 'hate', 'awful', 'horrible', 'dislike', 'unpleasant', 'miserable', 'displeasing', 'disgusting', 'annoying', 'atrocious', 'abysmal', 'dreadful', 'grim', 'lousy', 'unfortunate', 'worst', 'poor', 'inferior', 'mediocre', 'not_good', 'not_happy', 'not_excellent', 'not_love', 'not_awesome', 'not_amazing', 'not_beautiful', 'not_wonderful', 'not_fantastic', 'not_terrible', 'not_nice', 'not_positive', 'not_great', 'unlikely', 'not_outstanding', 'not_superb', 'not_delightful', 'not_joyful', 'not_cheerful', 'not_splendid', 'not_best', 'not_top-notch', 'deplorable', 'detestable', 'abominable', 'noxious', 'repulsive', 'offensive', 'disastrous', 'mournful', 'tragic', 'heartbreaking', 'woeful', 'pitiful', 'gloomy', 'lamentable', 'sadness', 'regrettable', 'grief', 'lament', 'sorrowful', 'unfortunate', 'mourn', 'forlorn', 'desolate', 'dismal', 'melancholy', 'lugubrious', 'suffering', 'anguish', 'dolorous', 'piteous', 'heart-rending', 'piteous'],
    0: ['using', 'and', 'about', 'second', 'word', 'god', 'oh', 'my', 'you', 'become', 'lord', 'chicken', 'the', 'is', 'it', 'are', 'in', 'on', 'at', 'this', 'that', 'with', 'by', 'for', 'to', 'from', 'of', 'as', 'an', 'be', 'or', 'not', 'and', 'but', 'can', 'will', 'would', 'should', 'could', 'may', 'might', 'must', 'shall', 'all', 'some', 'any', 'every', 'each', 'no', 'yes', 'nope', 'maybe', 'perhaps', 'possibly', 'probably', 'certainly', 'definitely', 'absolutely', 'indeed', 'never', 'always', 'sometimes', 'occasionally', 'frequently', 'rarely', 'seldom', 'hardly', 'barely', 'usually', 'generally', 'normally', 'often', 'everywhere', 'nowhere', 'somewhere', 'anywhere', 'anyhow', 'anyway', 'somehow', 'otherwise']
}

def perform_sentiment_analysis(text):
    # Check if text is a valid string
    if isinstance(text, str):
        # Sentiment analysis using TextBlob
        analysis = TextBlob(text)

        # Determine whether sentiment is positive, negative, or neutral
        if analysis.sentiment.polarity > 0:
            return 'Positif'
        elif analysis.sentiment.polarity < 0:
            return 'Negatif'
        else:
            return 'Netral'
    else:
        # Return 'Netral' for non-string values
        return 'Netral'

def add_sentiment_columns(df):
    # Fungsi untuk menambahkan kolom sentimen dan sentimen_encoded ke DataFrame
    df['Sentimen'] = df['Cleaned Comment'].apply(perform_sentiment_analysis)
    df['Sentimen_Encoded'] = df['Sentimen'].map({'Positif': 1, 'Negatif': -1, 'Netral': 0})
    return df

# Fungsi untuk menghitung skor sentimen berdasarkan kamus kata-kata positif dan negatif
def calculate_sentiment_score(comment, sentiment_weights):
    score = 0
    for word in comment.split():
        if word in sentiment_weights:
            score += sentiment_weights[word]
    return score

# Fungsi untuk menerjemahkan teks ke bahasa target
def translate_text(text, target_language='en'):
    translator = Translator()
    translation = translator.translate(text, dest=target_language)
    return translation.text

"""## **Proccess Sentimen Analysis (Model BERT)**"""

# Ambang batas negatif - netral - positif
threshold = 0.5

# Inisialisasi pipeline untuk analisis sentimen menggunakan BERT
sentiment_analyzer = pipeline("sentiment-analysis")

# Membersihkan Output Terminal
clear_output()

# Path ke file Excel setelah proses pembersihan
cleaning_file = path.join(hasil_scrapping_directory, selected_channel, f"DataCleaning_{selected_channel}.xlsx")

# Membaca DataFrame dari file Excel
df_cleaned = read_excel(cleaning_file)

# Menambahkan kolom sentimen ke DataFrame
df_with_sentiment = add_sentiment_columns(df_cleaned)

# Simpan DataFrame yang sudah dianalisis ke file Excel
analyzed_file_path = path.join(hasil_scrapping_directory, selected_channel, f"DataAnalysis_{selected_channel}.xlsx")

# Data yang sudah dibersihkan dari DataFrame
cleaned_comments = df_with_sentiment['Cleaned Comment'].astype(str).tolist()

# Gunakan model BERT untuk menganalisis sentimen
results_bert = sentiment_analyzer(cleaned_comments)

# Tambahkan hasil analisis sentimen BERT ke DataFrame
df_with_sentiment['Sentimen_BERT'] = [
    'NETRAL' if row.Sentimen == 'Netral' else (
        result['label'] if (result['label'] != 'POSITIVE' and result['label'] != 'NEGATIVE') or result['score'] > threshold else 'NETRAL'
    )
    for index, (row, result) in enumerate(zip(df_with_sentiment.itertuples(), results_bert))
]

# Simpan DataFrame yang sudah dianalisis ke file Excel
df_with_sentiment.to_excel(analyzed_file_path, index=False)

banner(2)
print(f"Data yang sudah dianalisis sentimen berhasil disimpan ke {analyzed_file_path}")

"""## **Proccess Mapping Sentimen**"""

# Path ke file Excel hasil analisis sentimen
analyzed_file_path = os.path.join(hasil_scrapping_directory, selected_channel, f"DataAnalysis_{selected_channel}.xlsx")

# Membaca DataFrame dari file Excel
df_analyzed = pd.read_excel(analyzed_file_path)

# Mapping label sentimen antara 'Sentimen' dan 'Sentimen_BERT'
mapping_sentimen = {'Positif': 'POSITIVE', 'Negatif': 'NEGATIVE', 'Netral': 'NEUTRAL'}

# Mengubah nilai pada kolom 'Sentimen' sesuai dengan mapping
df_analyzed['Sentimen'] = df_analyzed['Sentimen'].map(mapping_sentimen)

# Membandingkan antara kolom 'Sentimen' dan 'Sentimen_BERT'
matching_data = df_analyzed[df_analyzed['Sentimen'] == df_analyzed['Sentimen_BERT']]

# Menyimpan data yang memiliki label sentimen yang sama
matching_file_path = os.path.join(hasil_scrapping_directory, selected_channel, f"MatchDataAnalysis_{selected_channel}.xlsx")
matching_data.to_excel(matching_file_path, index=False)

print(f"Data yang memiliki label sentimen yang sama disimpan ke {matching_file_path}")

"""## **Display (Pie Diagram) Result of Sentimen Group By [Negatif, Netral, Positif]**

"""

# Hitung persentase masing-masing sentimen
sentimen_counts = df_with_sentiment['Sentimen'].value_counts(normalize=True) * 100
sentimen_counts2 = df_with_sentiment['Sentimen_BERT'].value_counts(normalize=True) * 100

# Plot pie chart untuk sentimen
plt.figure(figsize=(16, 8))
plt.subplot(1, 2, 1)
plt.pie(sentimen_counts, labels=sentimen_counts.index, autopct='%1.1f%%', startangle=90, colors=['green', 'red', 'gray'])
plt.title('Persentase Sentimen')

# Plot pie chart untuk sentimen BERT
plt.subplot(1, 2, 2)
plt.pie(sentimen_counts2, labels=sentimen_counts2.index, autopct='%1.1f%%', startangle=90, colors=['green', 'red', 'gray'])
plt.title('Persentase Sentimen BERT')

plt.show()

"""## **Configuration Visualization Wordcloud from Comment**"""

# Path ke file Excel hasil analisis sentimen
analyzed_file_path = path.join(hasil_scrapping_directory, selected_channel, f"DataAnalysis_{selected_channel}.xlsx")

# Membaca DataFrame dari file Excel
df_analyzed = read_excel(analyzed_file_path)

# Mengganti nilai NaN dengan string kosong ('') pada kolom "Cleaned Comment"
df_analyzed['Cleaned Comment'] = df_analyzed['Cleaned Comment'].fillna('')

# Membuat subset data untuk tiap kategori sentimen
df_neutral = df_analyzed[df_analyzed['Sentimen'] == 'Netral']
df_negative = df_analyzed[df_analyzed['Sentimen'] == 'Negatif']
df_positive = df_analyzed[df_analyzed['Sentimen'] == 'Positif']

# Menggabungkan semua kata dalam kolom "Cleaned Comment" untuk tiap kategori sentimen
text_neutral = ' '.join(df_neutral['Cleaned Comment'])
text_negative = ' '.join(df_negative['Cleaned Comment'])
text_positive = ' '.join(df_positive['Cleaned Comment'])

# Membuat objek WordCloud untuk tiap kategori sentimen
wordcloud_neutral = WordCloud(width=800, height=400).generate(text_neutral)
wordcloud_negative = WordCloud(width=800, height=400).generate(text_negative)
wordcloud_positive = WordCloud(width=800, height=400).generate(text_positive)

"""### **Display Netral Sentimen - Wordcloud**"""

# Menampilkan wordcloud untuk tiap kategori sentimen secara terpisah
plt.figure(figsize=(8, 4))
plt.imshow(wordcloud_neutral, interpolation='bilinear')
plt.title('Wordcloud - Sentimen Netral')
plt.axis('off')
plt.show()

"""### **Display Negatif Sentimen - Wordcloud**"""

plt.figure(figsize=(8, 4))
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.title('Wordcloud - Sentimen Negatif')
plt.axis('off')
plt.show()

"""### **Display Positif Sentimen - Wordcloud**"""

plt.figure(figsize=(8, 4))
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.title('Wordcloud - Sentimen Positif')
plt.axis('off')
plt.show()